{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28528\n",
      "7378\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from sys import exit\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import string\n",
    "\n",
    "def constructtrial(path):\n",
    "    \"\"\"Gets data and splits text to tokens and puts them into a list\"\"\"\n",
    " \n",
    "    newfile = []\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            filepath = subdir + os.sep + filename\n",
    "            if filepath.endswith(\".xml\"):\n",
    "                root = ET.parse(filepath)\n",
    "                t = root.find('TEXT')\n",
    "                text = t.text\n",
    "                doc = nlp(text)\n",
    "                for token in doc:\n",
    "                    newfile += [token.text]\n",
    "    return newfile\n",
    "\n",
    "\n",
    "def construct(attributes, path):\n",
    "    \"\"\"constructs a list of lists with entries text, iso, and all given attributes\"\"\"\n",
    "    \n",
    "    isoents = ['PLACE','PATH','SPATIAL_ENTITY','SPATIAL_SIGNAL',\\\n",
    "              'MOTION','MOTION_SIGNAL','MEASURE','NONMOTION_EVENT']\n",
    "    newfile = []\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            filepath = subdir + os.sep + filename\n",
    "            if filepath.endswith(\".xml\"):\n",
    "                root = ET.parse(filepath)\n",
    "\n",
    "                t = root.find('TEXT')\n",
    "                text = t.text\n",
    "                doc = nlp(text)\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    for ent in doc.ents:\n",
    "                        retokenizer.merge(doc[ent.start:ent.end])\n",
    "    return [x.lemma_ for x in doc if not x.is_punct and not x.is_stop]\n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "                doc = nlp(root[0].text)\n",
    "\n",
    "                for token in doc:\n",
    "                    found = False\n",
    "                    for elem in root:\n",
    "                        for i in isoents:\n",
    "                            for subelem in elem: \n",
    "                                if subelem.tag == i:\n",
    "                                    if subelem.get('start') == str(token.idx):\n",
    "                                        txt = subelem.get('text')\n",
    "                                        # split the token if it consists of \n",
    "                                        # more than one word\n",
    "                                        x = txt.split(\" \")\n",
    "                                        for e in range(len(x)):\n",
    "                                            newfile += [[x[e],i]+[subelem.get(attributes[a]) for a in range(len(attributes))]]  \n",
    "                                            # use this instead when you want to put a specific string if an attribute is None:\n",
    "                                            #newfile += [[x[e],i]+[subelem.get(attributes[a]) if subelem.get(attributes[a])!=None  else '' for a in range(len(attributes))]] \n",
    "                                        found = True\n",
    "                                        break\n",
    "                    if not found: \n",
    "                        #filter out useless data\n",
    "                        #txt = (str(token.text)).replace('\\s','')\n",
    "                        txt = (str(token.text)).translate(str.maketrans('', '', string.whitespace))\n",
    "                        if (txt == '' or txt == ' '): \n",
    "                            pass\n",
    "                        else: \n",
    "                            newfile += [[txt ,'O']+[None for n in range(len(attributes))]]\n",
    "                            \n",
    "    return newfile\n",
    "\n",
    "def writedata(path,fieldnames,datalist):\n",
    "    \"\"\"\"writes columns and data in a csv file\"\"\"\n",
    "    \n",
    "    with open(path, 'w', newline='',encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for each in range(0,len(datalist)):\n",
    "            writer.writerow({fieldnames[f]: datalist[each][f] for f in range(len(fieldnames))})\n",
    "            \n",
    "            \n",
    "attributes = ['dimensionality','form','motion_type','motion_class',\\\n",
    "              'motion_sense','semantic_type','motion_signal_type']\n",
    "columnnames = ['text', 'iso'] + attributes\n",
    "\n",
    "# construct list of lists that will later be converted to csv\n",
    "trialLst = constructtrial(r'C:\\Users\\anika\\Desktop\\Data\\spaceeval_trial_data') # dev = trial\n",
    "trainSetClass = construct(attributes, r'C:\\Users\\anika\\Desktop\\Data\\Training')\n",
    "testSetClass = construct(attributes, r'C:\\Users\\anika\\Desktop\\Data\\test_task8\\Test.configuration3')\n",
    "\n",
    "print(len(trainSetClass))\n",
    "print(len(testSetClass))\n",
    "\n",
    "# write data into csv files\n",
    "writedata('train_new.csv',columnnames,trainSetClass)\n",
    "writedata('test_new.csv',columnnames,testSetClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
